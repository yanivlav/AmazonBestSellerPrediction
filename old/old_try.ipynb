{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 102.0.5005\n",
      "Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "Driver [C:\\Users\\Yaniv\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n",
      "C:\\Users\\Yaniv\\AppData\\Local\\Temp/ipykernel_35328/3755370276.py:57: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml.parser. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35328/3755370276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"window.scrollTo(0, document.body.scrollHeight);\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# sleep_between_interactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mitemsLinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a[tabindex=\"-1\"]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mpage2Links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//li[contains(@class,'a-normal')]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Yaniv\\anaconda3\\envs\\datascience\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mbuilder_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuilder_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 raise FeatureNotFound(\n\u001b[0m\u001b[0;32m    246\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml.parser. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Creating the basic URL of Amazon Best Sellers page ----------------------------------------------------------------\n",
    "\n",
    "URL = 'https://www.amazon.com/Best-Sellers/zgbs'\n",
    "header = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36',\n",
    "          'referer': 'https://www.amazon.com/'}\n",
    "html = requests.get(URL, headers=header)\n",
    "soup = BeautifulSoup(html.content, 'html.parser')\n",
    "category = soup(\"div\", attrs={\n",
    "                'class': 'p13n-zg-nav-tree-all_style_zg-browse-item1rdKf _p13n-zg-nav-tree-all_style_zg-browse-height-small_nleKL'})\n",
    "\n",
    "#p13n-zg-nav-tree-all_style_zg-browse-item1rdKf _p13n-zg-nav-tree-all_style_zg-browse-height-small_nleKL\n",
    "\n",
    "# creating all the necessery links for the categories ----------------------------------------------------------------\n",
    "\n",
    "categoryNames = list()\n",
    "Links = list()\n",
    "categoryLinks = list()\n",
    "categoryURL = list()\n",
    "\n",
    "for name in category:\n",
    "    categoryNames.append(name.get_text())\n",
    "for link in soup.find_all('a'):\n",
    "    Links.append(link.get('href'))\n",
    "for i in Links[190:207]:\n",
    "    categoryLinks.append(i)\n",
    "for i in Links[208:230]:\n",
    "    categoryLinks.append(i)\n",
    "for i in categoryLinks:\n",
    "    categoryURL.append('https://www.amazon.com' + i)\n",
    "\n",
    "# creating all the necessery links for the products ----------------------------------------------------------------\n",
    "\n",
    "header2 = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36',\n",
    "           'referer': 'https://www.amazon.com/Best-Sellers-Amazon-Devices-Accessories/zgbs/amazon-devices/ref=zg_bs_nav_0'}\n",
    "itemsUrl = list()\n",
    "itemsLinks = list()\n",
    "itemsUrl2 = list()\n",
    "page2Links = list()\n",
    "page2URL = list()\n",
    "\n",
    "# print(\"https://www.amazon.com/Best-Sellers-Amazon-Devices-Accessories/zgbs/amazon-devices/ref=zg_bs_pg_2?_encoding=UTF8&pg=2\")\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "for url in categoryURL:\n",
    "    response = requests.get(url, headers=header2)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    itemsLinks = soup(\"a\", attrs={\"tabindex\": \"-1\"})\n",
    "    page2Links = soup.find(\"li\", attrs={\"class\": \"a-normal\"})\n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # sleep_between_interactions\n",
    "    soup = BeautifulSoup(driver, \"lxml\")\n",
    "    itemsLinks = driver.find_element_by_css_selector('a[tabindex=\"-1\"]').get_attribute('href') \n",
    "    page2Links = driver.find_elements_by_xpath(\"//li[contains(@class,'a-normal')]\").get_attribute('href')\n",
    "    print(itemsLinks)\n",
    "    print(page2Links)\n",
    "\n",
    "    \n",
    "    # for i in itemsLinks[1:]:\n",
    "    #     itemsUrl.append('https://www.amazon.com' + i['href'])\n",
    "    # for i in page2Links:\n",
    "    #     page2URL.append('https://www.amazon.com' + i.get('href'))\n",
    "\n",
    "\n",
    "for url in page2URL:\n",
    "    # response = requests.get(url, headers=header2)\n",
    "    # soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # page2Links = soup(\"a\", attrs={\"tabindex\": \"-1\"})\n",
    "    driver.get(url)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # sleep_between_interactions\n",
    "    page2Links = driver.find_elements_by_xpath(\"//li[contains(@class,'a-normal')]\")\n",
    "    for i in page2Links[1:]:\n",
    "        itemsUrl.append('https://www.amazon.com' + i['href'])\n",
    "\n",
    "print(len(itemsUrl))\n",
    "for i in itemsUrl:\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "631f2da9300be9763c746115158d7fda57bb43f84de6a8c2c3474ff6e3fd2b9c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
